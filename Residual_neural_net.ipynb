{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Transformed from linux machine for simpler setup:\n",
        "Contains a simple implementation of cipher text processing residual neural network."
      ],
      "metadata": {
        "id": "VzaqfsiyiRYG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjxJDBSUiQsi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency: how many 1s in the sequence\n",
        "runs: length of the longest sub-sequence of 1 in the sequence\n",
        "serial: for n in range(length): test how many matches in \n"
      ],
      "metadata": {
        "id": "gXV3Vr5BMayX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#activation function types\n",
        "#In original paper, they used relu activation\n",
        "#Different activation function is used for parameter tuning\n",
        "def activation_func(activation):\n",
        "    return  nn.ModuleDict([\n",
        "        ['relu', nn.ReLU(inplace=True)],\n",
        "        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
        "        ['selu', nn.SELU(inplace=True)],\n",
        "        ['none', nn.Identity()]\n",
        "    ])[activation]"
      ],
      "metadata": {
        "id": "yoM9DvICk7Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.serialization import INT_SIZE\n",
        "'''\n",
        "From Xia et.al's implementation, the residual network shows as below\n",
        "convolutional 1d layer\n",
        "batch normalization layer\n",
        "activation layer\n",
        "'''\n",
        "class ResBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, size,activation='relu',downsampling=1):\n",
        "        super().__init__()\n",
        "        #paper structure\n",
        "        self.conv1 = nn.Conv1d(size, size, 1, padding=0,stride=downsampling)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(size)\n",
        "        self.activate = activation_func(activation)\n",
        "        #apply identity activation to uniform the sizes\n",
        "        self.blocks=nn.Identity()\n",
        "    def activation(self, x):\n",
        "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
        "        return x\n",
        "   \n",
        "    \"\"\"\n",
        "    Combine output with the original input\n",
        "    \"\"\"\n",
        "    def forward(self, x): \n",
        "      return x+ self.activation(x) "
      ],
      "metadata": {
        "id": "VEOgOzKJjHst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A ResNet layer composed by `n` blocks stacked one after the other\n",
        "    \"\"\"\n",
        "    def __init__(self, channels, block=ResBlock, n=1, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.Sequential(\n",
        "            block(channels ,*args, **kwargs),\n",
        "            *[block(channels, *args, **kwargs) for _ in range(n - 1)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blocks(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4KQxLxTIWQpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet encoder composed by layers with increasing features.\n",
        "    \"\"\"\n",
        "    def __init__(self, blocks_size,\n",
        "                 activation='relu', block=ResBlock,n=1, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.blocks_size = blocks_size\n",
        "        \n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Conv1d(blocks_size, blocks_size,1,padding=0),\n",
        "            nn.BatchNorm1d(self.blocks_size),\n",
        "            activation_func(activation)\n",
        "        )\n",
        "        \n",
        "        self.blocks = nn.ModuleList([ \n",
        "            ResNetLayer(blocks_size, n=n, activation=activation, \n",
        "                        *args, **kwargs),\n",
        "            *[ResNetLayer(blocks_size, \n",
        "                         n=n, activation=activation, \n",
        "                          *args, **kwargs) for _ in range(n - 1)]       \n",
        "        ])\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.gate(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "nZ98oaIcXi_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    This class represents the tail of ResNet. It performs a global pooling and maps the output to the\n",
        "    correct class by using a fully connected layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, n_classes):\n",
        "        super().__init__()\n",
        "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.decoder = nn.Linear(in_features, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tUvIbKvMZouT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = ResNetEncoder(3,n=5)\n",
        "input = torch.randn(3,3,4)\n",
        "output = m(input)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFn_ukp2DwEW",
        "outputId": "fe3ff0bc-956e-4485-ae45-3e330f69643a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 3.1906,  4.6163,  5.3328,  3.3847],\n",
              "         [ 7.4579,  7.0185,  6.3493,  7.3638],\n",
              "         [ 5.4903,  6.8321,  5.2643,  5.3586]],\n",
              "\n",
              "        [[ 3.4586, 37.5507,  3.1800,  3.3623],\n",
              "         [ 8.0851, 17.0998,  7.4518,  7.3301],\n",
              "         [ 6.2925, 24.4898,  5.4314,  5.3373]],\n",
              "\n",
              "        [[ 3.1478, 34.1901,  8.2235,  3.3187],\n",
              "         [ 7.5069, 22.5154,  5.9799,  7.2647],\n",
              "         [ 5.4272, 32.6679,  6.2005,  5.2969]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Feature Engineering\n",
        "'''\n"
      ],
      "metadata": {
        "id": "Yaz5ar4jkDSz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}